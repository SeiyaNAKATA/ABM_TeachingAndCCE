# -*- coding: utf-8 -*-"""Network_search by reinforcement learningrecord Q and W valueoutput: "Steps",        "Goal",        "Round",        "Generation",        "Teaching_phase",        "Chain""""import numpy as npimport pandas as pdimport copyimport csvimport networkx as nx"""def function"""def Choose(Q_list,Beta=1):    """    choose behavior function from the Roth-Erev model    this is one of softmax fuction deciding which behavior should be taken depending with each Q    return a probabilities_list Pr    Pr(Q_1) = exp(Q_1) / sigma(exp(Q_i))    """       eQ_list = np.exp(Q_list) * Beta    Pr = eQ_list / sum(eQ_list)    return Pr #uesd as weight of np.random.choice()def Q_Update(Q_list,pre,current,reward,Alpha=0.9):    """    update function following the reinforcement learning.    Q_list: Q_value in Q_list(dictionary) is abstracted    pre: pre_state is changed to tuple in this fuction    action: the choosen action    reward: the rewad of the state    Alpha: learning rate. initial value is 0.9    """    Q_list[pre,current] = (1-Alpha) * Q_list[pre,current] + Alpha * rewarddef W_Update(W_list,pre,current,Alpha=0.9):    """    update function following the reinforcement learning.    W_list: W_value in W_list(dictionary) is abstracted    pre: pre_state is changed to tuple in this fuction    current: current_state is changed to tuple in this fuction    Alpha: learning rate. initial value is 0.9    """    W_list[pre] = (1-Alpha) * W_list[pre] + Alpha * W_list[current]#______________________________________________________________________________"""setting parameter"""import syscommand_arg = sys.argv    # read commandline argumentprint('command_arg=[file_name,Network,Generation,Round,T_rate,Chain_id]')file_name = command_arg[0]Network = str(command_arg[1])print('Network=',Network)Generation = int(command_arg[2])print('Generation=',Generation)Round = int(command_arg[3])print('Round=',Round)T_rate = int(command_arg[4])print('T_rate=',T_rate)Chain_id=int(command_arg[5])print('Chain_id=',Chain_id)Beta = 1.0 # parameter of ChooseAlpha = 0.9 # parameter of Q_UpdateLimit = 1000 # Limit number of steps in a roundTeaching_phase = int(Round*T_rate/10)"""setting netwrok"""df = pd.read_csv(Network, header=None)edge_list = df.values.tolist()edge_list = list(map(lambda x: tuple(x), edge_list))G_directed = nx.DiGraph()G_undirected = nx.Graph()G_directed.add_edges_from(edge_list)G_undirected.add_edges_from(edge_list)n_nodes = len(G_directed.nodes)edge_dict = {}for i in range(n_nodes): # set keys    edge_dict[i]=[]for i in range(len(df)): # set values    edge_dict[int(df[0][i])].append(int(df[1][i]))start = 0bridges = np.ravel(list(nx.bridges(G_undirected)))nx.shortest_path_length(G_directed,source=start)goals = {2:1, 6:4, 23:9, 31:16, 42:25, 43:36}#______________________________________________________________________________import osdir_name = f"{Network}_G{Generation}R{Round}T{Teaching_phase}"os.makedirs(f"{dir_name}", exist_ok=True)os.makedirs(f"Qt{dir_name}", exist_ok=True)os.makedirs(f"Qr{dir_name}", exist_ok=True)os.makedirs(f"Wt{dir_name}", exist_ok=True)os.makedirs(f"Wr{dir_name}", exist_ok=True)#______________________________________________________________________________"""run"""n_edges = len(edge_list)teaching_path_list = np.array([-1]*Limit)df_steps = pd.DataFrame(columns=["Steps","Goal","Round","Generation","Teaching_phase","Chain"])for g in range(Generation):    steps_list = np.array([None for i in range(Round)])    goals_list = np.array([None for i in range(Round)])    """initializing"""    Q = dict(zip(edge_list,[0.0]*n_edges))    # state_behavior_value list. Q={edge:q},...}, edge=(s,s')    W = dict(zip([n for n in range(n_nodes)],[0.0]*n_nodes))    W.update(goals)        for r in range(Round):        """initializing"""        steps = 0        current = start                while True:            """initializing and setting"""            choices = [] # choices of path rooted from the current node            Q_list = []            reward = 0            """explore"""            if g == 0: # In 1st generation agent always does Individual learning                steps += 1                pre = current                choices = edge_dict[current]                Q_list = [Q[current,x] for x in choices]                weight = Choose(Q_list,Beta)                current = np.random.choice(choices,p=weight)                if r == Round-1:                    teaching_path_list[steps-1] = current                """get reward"""                reward = W[current]                """update value"""                Q_Update(Q,pre,current,reward,Alpha)                W_Update(W,pre,current,Alpha)                """back to initial"""                if steps == Limit:                    steps_list[r] = steps                    goals_list[r] = current                    break                if current in goals.keys():                    steps_list[r] = steps                    goals_list[r] = current                    break                            else: # Teaching                if r < Teaching_phase:                    for e in learning_list:                        steps += 1                        pre = current                        current = e                                                """get reward"""                        reward = W[current]                                                """update value"""                        Q_Update(Q,pre,current,reward,Alpha)                        W_Update(W,pre,current,Alpha)                    steps_list[r] = steps                    goals_list[r] = current                    break                                else:#Individual learning                    steps += 1                    pre = current                    choices = edge_dict[current]                    Q_list = [Q[current,x] for x in choices]                    weight = Choose(Q_list,Beta)                    current = np.random.choice(choices,p=weight)                    if r == Round-1:                        teaching_path_list[steps-1] = current                                        """get reward"""                    reward = W[current]                                        """update value"""                    Q_Update(Q,pre,current,reward,Alpha)                    W_Update(W,pre,current,Alpha)                                        """back to initial"""                    if steps == Limit:                        steps_list[r] = steps                        goals_list[r] = current                        break                    if current in goals.keys():                        steps_list[r] = steps                        goals_list[r] = current                        break                            #save Q and W vector after teching or after final round        if r == Teaching_phase-1:            Q_list = list(Q.values())            with open(f'Qt{dir_name}/Qt{dir_name}_{Chain_id}.csv', 'a') as f:                writer = csv.writer(f)                writer.writerow(Q_list)            W_list = list(W.values())            with open(f'Wt{dir_name}/Wt{dir_name}_{Chain_id}.csv', 'a') as f:                writer = csv.writer(f)                writer.writerow(W_list)                        elif r == Round-1:            Q_list = list(Q.values())            with open(f'Qr{dir_name}/Qr{dir_name}_{Chain_id}.csv', 'a') as f:                writer = csv.writer(f)                writer.writerow(Q_list)            W_list = list(W.values())            with open(f'Wr{dir_name}/Wr{dir_name}_{Chain_id}.csv', 'a') as f:                writer = csv.writer(f)                writer.writerow(W_list)        else:            pass    df_temp = pd.DataFrame({"Steps":steps_list,                            "Goal":goals_list,                            "Round":[r+1 for r in range(Round)],                            "Generation":g+1,                            "Teaching_phase":Teaching_phase,                            "Chain":Chain_id})    df_steps = df_steps.append(df_temp, ignore_index=True)    learning_list = copy.deepcopy(teaching_path_list[0:steps])df_steps.to_csv(f"{dir_name}/{dir_name}_{Chain_id}.csv",index=False)